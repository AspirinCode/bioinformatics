{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02: Introduction to Next-generation sequencing (NGS)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will illustrate the properties of the NGS data and the pipeline to analyze the data. Let's have our feet wet with the sequencing data, and begin to map the reads against the genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Next generation sequencing (a.k.a high-throughput sequencing), a term used to describe a set of the state-of-the-art sequencing technologies, has revolutionalize the modern investigation of molecular biology:\n",
    "- **Illumina** (Solexa) Sequencing\n",
    "- **Roche 454** Sequencing\n",
    "- **ABI SOLiD** Sequencing\n",
    "- **Ion Torrent**: Proton/PGM Sequencing\n",
    "\n",
    "The reads can be either single-end or paired-end (mate pair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Advantages of NGS\n",
    "\n",
    "- Shorter read length and more repeats\n",
    "\n",
    "- Much higher efficiency: massively parallel, high-throughput\n",
    "\n",
    "- Lower cost\n",
    "\n",
    "- Higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Key Tasks in NGS Sequence Analysis\n",
    "\n",
    "- Sequence formats and quality assessment (序列格式与序列质量评估).\n",
    "- Mapping of NGS to reference sequences (比对到参考基因组序列).\n",
    "- Identification and labelling of variants, such as SNPs and structural variations (发现序列中变异如SNVs和SVs).\n",
    "- Sequence assembly (序列拼接).\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Raw Sequencing Data\n",
    "\n",
    "- **FASTQ** : name + sequence +  associated **per-base Phred quality score**.\n",
    "\n",
    "- Here is an example:\n",
    "```\n",
    "@SEQ_ID\n",
    "GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n",
    "+\n",
    "!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n",
    "```\n",
    "\n",
    "#### Illumina sequence identifiers\n",
    "\n",
    "Sequences from the Illumina software use a systematic identifier:\n",
    "```\n",
    "@EAS139:136:FC706VJ:2:2104:15343:197393/1:Y:18:ATCACG\n",
    "```\n",
    "which contains the following information:\n",
    "\n",
    "| **Term** | **Description** |\n",
    "| --- | --- |\n",
    "| **EAS139** |\tthe unique instrument name |\n",
    "| **136**\t| the run id |\n",
    "| **FC706VJ** |\tthe flowcell id |\n",
    "| **2** | flowcell lane |\n",
    "| **2104** | tile number within the flowcell lane |\n",
    "| **15343** | 'x'-coordinate of the cluster within the tile |\n",
    "| **197393** |\t'y'-coordinate of the cluster within the tile |\n",
    "| **1** | the member of a pair, 1 or 2 (paired-end or mate-pair reads only) |\n",
    "| **Y**\t | Y if the read is filtered, N otherwise |\n",
    "| **18** | 0 when none of the control bits are on, otherwise it is an even number |\n",
    "| **ATCACG** | index sequence |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 PHRED score system\n",
    "\n",
    "- A Phred score of a base is\n",
    "$$\n",
    "Q_{phred} = -10 \\log_{10} e\n",
    "$$\n",
    "where $e$ is the estimated probability of a base being an sequencing error.\n",
    "\n",
    "- **Phred+33**: Illumina 1.8+, converted to ASCII corresponding to $Q_{phred}+33$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 1</font>\n",
    "\n",
    "1. Write a Python script to convert the **PHRED+33 ASCII-encoded quality string** to the corresponding estimated probability of the base being a sequencing error.\n",
    "\n",
    "2. Given a FASTQ-formatted read below, can you compute the probability that the sequenced read is completely correct? Is this a good or bad read?\n",
    "```\n",
    "@IL4_315:7:105:408:43\n",
    "ATTTGGCTCTCTGCTTGTTTATTATTGGTGTATNGG\n",
    "+\n",
    "+1,1+16;>;166>;>;;>>;>>>>>>,>>>>>+>>\n",
    "```\n",
    "3. Draw a histogram of the overall quality distribution in the file `data/example.fq`.\n",
    "\n",
    "4. Use boxplot to illustrate the quality distribution for each sites across the reads in the file `example.fq`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Phred33ToQ(qual):\n",
    "    \"\"\"convert the qual character into the quality value.\"\"\"\n",
    "    return ord(qual)\n",
    "\n",
    "def QToErrorRate(Q):\n",
    "    \"\"\"convert the quality value to the error probability.\"\"\"\n",
    "    pass\n",
    "\n",
    "def ReadFastqFile(fname):\n",
    "    \"\"\"Read a fastq file and return a list of sequences and corresponding qualities\"\"\"\n",
    "    seqs = []\n",
    "    quals = []\n",
    "    fh = open(fname)\n",
    "    while True:\n",
    "        fh.readline()\n",
    "        seq = fh.readline().rstrip()\n",
    "        fh.readline()\n",
    "        qual = fh.readline().rstrip()\n",
    "        if len(seq)==0:\n",
    "            break\n",
    "        seqs.append(seq)\n",
    "        quals.append(qual)\n",
    "    return seqs, quals\n",
    "\n",
    "def QToHistogram(quals):\n",
    "    \"\"\"Convert a list of qualities into a histogram.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-8bf80f31ba80>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-8bf80f31ba80>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    plt.boxplot(hist))\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "seqs, quals = ReadFastqFile(\"data/example.fq\")\n",
    "quals = Phred33ToQ(quals)\n",
    "hist = QToHistogram(quals)\n",
    "plt.bar(range(len(hist)), hist)\n",
    "plt.boxplot(hist)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](demo/boxplot.py) is a demo for drawing a boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mapping: Align the reads to the reference genome\n",
    "\n",
    "As a popular efficient algorithm in text matching and full-text query, now BWT has become one of the efficient tools in mapping reads to the reference genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Burrows-Wheeler transform (BWT)\n",
    "\n",
    "For a input string, how can we build the sorted Burrows-Wheeler array (BWA) and conduct the Burrows-Wheeler transform?\n",
    "\n",
    "It's very simple by\n",
    "- first **compute all the possible rotations of the string**, and \n",
    "- then **sort them lexically**. \n",
    "\n",
    "![](../images/bwt1.gif)\n",
    "\n",
    "Therefore, $BWA(T)$ is the lexical sorting of `T`, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bwa(t):\n",
    "    t = t + \"$\"\n",
    "    bwa = []\n",
    "    s0 = []\n",
    "    lt = len(t)\n",
    "    for i in range(lt):\n",
    "        bwa.append(t[i:] + t[:i])\n",
    "        s0.append((t[i:] + t[:i], i))\n",
    "    return sorted(bwa), sorted(s0)\n",
    "\n",
    "sa = {}\n",
    "rank = {}\n",
    "array = bwa(\"ACAACG\")[1]\n",
    "for i, s in enumerate(array):\n",
    "    print \"{}\\t{}\\t{}\".format(i, s[0], s[1])\n",
    "    sa[i] = s[1]\n",
    "    rank[s[1]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suffix_sorted = [array[i][0][:array[i][0].index('$')+1] for i in xrange(len(array))]\n",
    "for st in suffix_sorted:\n",
    "    print st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `sa` is the corresponding suffix array and `rank` is its reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in sa:\n",
    "    print i, sa[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in rank:\n",
    "    print _, rank[_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then write down the last column as the `BWT(T)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bwt(t):\n",
    "    bw = bwa(t)[0]\n",
    "    lt = len(t)\n",
    "    bwt = [seq[lt] for seq in bw]\n",
    "    \n",
    "    return ''.join(bwt)\n",
    "\n",
    "print bwt(\"ACAACG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reconstruct the sequence from the BWT string\n",
    "\n",
    "![](../images/bwt2.gif)\n",
    "![](../images/bwt3.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bwt_string = 'GC$AAAC'\n",
    "\n",
    "def occ(ch):\n",
    "    \"\"\"Compute number of characters that are literally smaller than ch\"\"\"\n",
    "    return sum([c<ch for c in bwt_string])\n",
    "\n",
    "def count(ch, idx):\n",
    "    \"\"\"Count the occurrences of ch in bwt before idx\"\"\"\n",
    "    return sum([c==ch for c in bwt_string[:idx]])\n",
    "\n",
    "def LF(ch,  idx):\n",
    "    \"\"\"Last-to-First conversion for ch at idx in bwt\"\"\"\n",
    "    return occ(ch) + count(ch, idx)\n",
    "\n",
    "def reconstruct(bwt_str):\n",
    "    \"\"\"Reconstruct the genome from the bwt\"\"\"\n",
    "    i = 0\n",
    "    t = ''\n",
    "    while not bwt_str[i] == '$':\n",
    "        t = bwt_str[i] + t\n",
    "        print i\n",
    "        i = LF(bwt_str[i],  i)\n",
    "        \n",
    "    return t\n",
    "\n",
    "seq = reconstruct(bwt_string)\n",
    "print seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/bwt4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full-text Minute-size index (FM Index)\n",
    "\n",
    "How do we convert a genome into an alternate representation that permits rapid matching of millions of sequence reads? FM-index and BWT, in my opinion, stands for the same thing, is used for creating index for a long text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read Alignment\n",
    "\n",
    "How can we use an FM index and BWT to rapidly align reads to a refernce genome?\n",
    "\n",
    "### 3.1 Exact matching with FM Index\n",
    "\n",
    "For a query sequence, we can use an iterative method to align:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bwtQuery(query, genome):\n",
    "    index = bwt( genome)\n",
    "    top = 0\n",
    "    bot = len(index)\n",
    "    for ch in query[::-1]:\n",
    "        top = LF(ch, top)\n",
    "        bot = LF(ch, bot)\n",
    "    return top, bot\n",
    "\n",
    "genome = \"ACAACG\"\n",
    "query = \"AAC\"\n",
    "hits = bwtQuery(query, genome)\n",
    "print \"Finally hit at {}\".format(hits)\n",
    "print \"Find {} at position {} of genome {}\".format(query, sa[hits[0]], genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Explanation of the algorithm</font>\n",
    "\n",
    "In each iteration, **top** and **bot** delimit the range of rows beginning with progressively longer suffixes of the `query` (i.e., `top` and `bot`defines the ranges of rows of Burrows-Wheeler Array (BWA) that exactly match the current suffix of the `query` reads), and suffix of the `query` extends a base longer in each iteration.  So if range becomes empty (that is, `top = bot`), the `query suffix` (and therefore the `query` itself) does not occur in the `genome`.\n",
    "\n",
    "So we can see that the time complexity of the matching is $\\mathcal{O}(m)$ where $m$ is the length of the reads. That is, the algorithm is linear complexity, right?\n",
    "\n",
    "So once we've built the index of the genome, we can use it again and again. We don't need to rebuild the index, right? You don't have to pay for the time to build this index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find the exact hits in the genome\n",
    "\n",
    "Now we know that our query has a hit in the genome,  but the question is, \n",
    "\n",
    "> ### where is the hit in the genome?\n",
    "\n",
    "The only fact that it matched row 1 of the Burrows-Wheeler array (BWA) does me absolutely no good at all. \n",
    "\n",
    "Now we have BWT, OCC, and Count, we can compute the LF (Last-to-First, mapping last column to first column) function. How can we do with the information to locate the hit in the genome?\n",
    "\n",
    "#### 3.2.1 Naive solution 1\n",
    "\n",
    "We can use the \"**walk-left**\" to walk back to the beginning of the genome (that is, the row ending with `$` sign) and the number of steps to take is exactly the **offset of hit** in the genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = hits[0]\n",
    "offset = 0\n",
    "while not bwt_string[i]=='$':\n",
    "    ch = bwt_string[i]\n",
    "    i = LF(ch, i)\n",
    "    offset += 1\n",
    "print offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 2</font>\n",
    "\n",
    "If we have multiple hits, that is, there are more than one row betwen `top` and `bottom`, say, 2 and 5. What should we do to obtain all the hits (offsets in the genome)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm, is also linear, but in the length of the genome, $\\mathcal{O}(n)$. Due to the large size of the genome, this could be too slow.\n",
    "\n",
    "But any other ideas can help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Naive solution 2: Rows to Reference Positions\n",
    "\n",
    "We can keep the whole suffix array in the memory. Thus it is a **lookup** in the array to find the reference position. That is, when we build the index, we need to store the offset for each row of the suffix array:\n",
    "> ### However, for human genome, the suffix array is $\\sim 12 GB$, which is too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Hybrid solution\n",
    "\n",
    "\n",
    "![](../images/bwt5.gif)\n",
    "Store not all, but sample of suffix array, and then \"walk left\" to the next sampled (a.k.a \"marked\") row to the left. This approach is proposed by Ferragina and Manzini in 2008. This, can cut down the storage demand.\n",
    "\n",
    "混合方法是上述两种策略的折中，只保存部分后缀数组，然后采用上述第一种“向左游走”策略，直到找到下一个“被标记”的后缀数组行为止。将“向左游走”的步数加上该行的“偏移量”，得到的结果就是输入检索序列在基因组上的偏移位置。\n",
    "\n",
    "Thus the final hit offset is the sum of the \"walk-left\" steps and the \"marked\" row offset of the sampled suffix array. \n",
    "\n",
    "One of the ngs short read aligner, Bowtie, \"marks\" every 32nd  row by default (but note that this can be modifiable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Improving the LF() function\n",
    "\n",
    "Now the alignment of short reads against the genome is linear. However, in the inner iteration, the `LF()` routine is still very time-consuming. `LF(ch, idx)`tries to determine the **rank** of *ch* in row $idx$, which means that we need to count the occurrences of `ch` in all previous rows. This is very expensive, although the rank is a very simple metric.\n",
    "\n",
    "![](../images/bwt6.gif)\n",
    "\n",
    "Instead of doing that, we can build a data structure that pre-calculate the cumulative counts for A/C/T/G up to **periodic checkpoints** in BWT. So when you need to compute the counts at an arbitrary point, you can go to the nearest checkpoint and make an adjustment by counting the number of characters betwen the index and the checkpoint. A very straight-forward approach, all right.\n",
    "\n",
    "![](../images/bwt7.gif)\n",
    "\n",
    "So now we know that the full index consists of \n",
    "- the **Burrows-Wheeler transform (BWT)** with the same size as $T$, \n",
    "- the **sampling of the suffix array** with around $50\\%$ of $T$\n",
    "- the **checkpoint** for the `LF()` function to make a constant time ($~15\\%$ of $T$)\n",
    "\n",
    "That's what is inside of an FM-Index. Therefore the size of full FM-index is about $1.65x$ the size of $T$.\n",
    "\n",
    "In Bowtie, \n",
    "- Two bits for the encoding of each base and no compression\n",
    "- 16 bytes checkpoint for every 448 characters\n",
    "- Use default suffix sampling rate\n",
    "\n",
    "We can see that it is very small, compared to things like **suffix trees ($>45x$)**, **suffix arrays ($>15x$) **, or even **other kinds of hash indexes ($>15x$) for looking for seeds**.\n",
    "\n",
    "So it's a very compact index, and thus it is very efficient. It is a very wonderful data structure, except that we have not dealt with mismatches yet, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### Oligomer counting\n",
    "- Healy J *et al*: Annotating large genomes with exact word matches. *Genome Res* 2003, 13(10):2306-2315.\n",
    "   \n",
    "#### Whole-genome alignment\n",
    "- Li H *et al*: Fast and accurate short read alignment with Burrows-Wheeler transform. *Bioinformatics* 2009, 25(14): 1754-1760 (bwa aligner)\n",
    "    \n",
    "#### Smith-Waterman alignment to large reference\n",
    "- Lam TW *et al*: Compressed indexing and local alignment of DNA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">$\\S$ Exercise 3</font>\n",
    "\n",
    "Please organize the above python scripts into a object-oriented fashion.\n",
    "\n",
    "```python\n",
    "class BWT():\n",
    "    \"\"\"add your code here\"\"\"\n",
    "    def __init__(self, bwt):\n",
    "        pass\n",
    "    \n",
    "    def build_bwt(genome):\n",
    "        pass\n",
    "        \n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "And also run some examples to verify your codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: How to deal with mismatches?\n",
    "\n",
    "Once we are trapped by `top==bot`, which means that the query does not exactly occur in the genome. But we can change the current character until it can go ahead. This is called the **backtracking**.\n",
    "\n",
    "This method can also be applied when we want to take into accout the sequencing quality of the reads. We know that for a sequenced read, it is often of the poor quality in the 3' end. But the above alignment are starting from the right-most bases, so this often results in false hits. So for the bases with poor quality, we need to change the state since we are not confident of the sequenced results.\n",
    "\n",
    "Additionaly, to deal with the 3'-end bias of sequencing error, we can use the \"mirror\" index of the genome and also reverse the query string in order to get a better result.\n",
    "\n",
    "All these wonderful technique, can overcome some of the limitations of backtracking. Unfortunately, this will of course increase the computational burden. This requires you to make a balance between these two metrics - accuracy and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When creating BWT, Heng Li's **bwa** does not sort the entire Burrow-Wheeler array. Instead, he use an insertion-based method to achieve a better efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. The Alignment Pipeline\n",
    "\n",
    "The typical pipeline for alignment contains the following 5 steps:\n",
    "\n",
    "### 4.1 Obtain the reads and do some quality checking (QC)\n",
    "\n",
    "This is the data propressing step. In this step, besides obtaining the reads data, you need to trim the adapters and the poor-quality bases/reads from the data.\n",
    "\n",
    "The typical tools for this step usually include:\n",
    "- **[SRA Toolkit](https://github.com/ncbi/sra-tools)**'s `fastq-dump` for extracting `*.fastq` out of the `*.sra` file;\n",
    "- **[FastQC](www.bioinformatics.babraham.ac.uk/projects/fastqc)** for quality checking of the `fastq` files;\n",
    "- **[Picard Tools](https://broadinstitute.github.io/picard)** for marking the duplicates;\n",
    "- **[Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)** for low-quality trimming of the reads;\n",
    "- **[FastX Toolkit](http://hannonlab.cshl.edu/fastx_toolkit)** for processing the reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Conduct the alignment (mapping)\n",
    "\n",
    "To accomplish the task, you need to finish the following jobs:\n",
    "- Index the genome sequence;\n",
    "- Align the reads to the genome;\n",
    "- Sort the alignments;\n",
    "- Merge the alignments;\n",
    "- Recalibrate the alignments;\n",
    "- Merge the libraries;\n",
    "- Index the final alignments. \n",
    "\n",
    "The typical tools for this step include:\n",
    "- The **alignment tools** (e.g. [bwa](http://bio-bwa.sourceforge.net/); [bowtie](bowtie-bio.sourceforge.net/); [SOAP](http://soap.genomics.org.cn/)) \n",
    "- **[samtools](http://samtools.sourceforge.net/)**: providing various utilities for manipulating alignments in the [SAM](http://samtools.sourceforge.net/SAM1.pdf) format, including sorting, merging, indexing and generating alignments in a per-position format. \n",
    "- **[GATK](https://software.broadinstitute.org/gatk/)**: offering a wide variety of tools with a primary focus on variant discovery and genotyping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 `bwa`\n",
    "\n",
    "- `bwa index  [-a bwtsw|div|is] [-c] <in.fasta>`\n",
    "    * **Burrows-Wheeler Transform** construction algorithm with \"bwtsw\" for vertebrate-size genomes and \"is\" for smaller genomes.\n",
    "    \n",
    "- `bwa aln [options] <prefix> in.fq`\n",
    "    * aligns each single-ended fastq file individually;\n",
    "    * `<prefix>` is the name of the reference file spcified by `bwa index`;\n",
    "    * `[options]` controls alignment parameters, scoring matrix, seed length, etc.\n",
    "\n",
    "- `bwa sampe <prefix> <in1.sai> <in2.sai> <in1.fq> <in2.fq>`\n",
    "    * generates pairwise alignment from both `<in1.sai>` and `<in2.sai>` files produced by `bwa aln`;\n",
    "    * for paired-ended reads stored in `<in1.fq>` and `<in2.fq>`;\n",
    "    * produces `SAM` outputs.\n",
    "\n",
    "- `bwa bwasw <prefix> <query.fa>`\n",
    "    * aligns long reads in `<query.fa>` to the reference named `<prefix>`;\n",
    "    * produces `SAM`-formatted outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">`bwa` usage notes</font>\n",
    "- `bwa` finds matches up to a preset **edit distance** (by default, for a 100 bases allowing 5 edits)\n",
    "- `bwa` use `-q` to set clipping for **quality threshold**, e.g. 20.\n",
    "- ** Ambiguous (non-ACGT)** bases are treated as **mismatch**.\n",
    "- **Parallelization** for speed:\n",
    "    * split data into 1Gb blocks;\n",
    "    * each 1Gb blocks takes around 8 hours.\n",
    "- Check for truncated BAM files (e.g. using `samtools flagstats`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 How to further improve the alignments?\n",
    "\n",
    "To improve the alignments generated by `bwa`, you need to do the following steps:\n",
    "- **Remove the duplicates in the library**, which can utilize such tools like **samtools** and **Picard**.\n",
    "- **Realignment around the indels** and **Base quality calibration** using **GATK**. \n",
    "\n",
    "\n",
    "### Library duplicate removal\n",
    "\n",
    "PCR amplification is essential in the sequencing of **low-input** DNA samples. But unfortunately, this will result in duplicate DNA fragments, which can further lead to false SNP calls.\n",
    "\n",
    "To remove the duplicates, you can first **identify the read pairs whose outer ends map to the same position** on the genome and then **remove all but one copy**. This can be done by the two similar commands:\n",
    "- `samtools rmdup`\n",
    "- `Picard/GATK MarkDuplicates`\n",
    "\n",
    "### Realignment\n",
    "\n",
    "Short indels in the sample relative to the reference can pose difficulties for alignment task since\n",
    "- indels near the ends of the reads are often not aligned correctly;\n",
    "- the aligners prefer naturally to introducing SNPs rather than indels.\n",
    "\n",
    "But how to do realignment?  Usually, the realignment algorithm works by\n",
    "- Input set of known indels (e.g., previously publised indel sites, dbSNPs, 1K-Genomes, or estimated directly from the alignments) and a BAM file;\n",
    "- At each site, model the indels and the reference haplotype, and select best fit with data;\n",
    "- Generate and modify new BAM files where the indels have been introduced through the realignment;\n",
    "- This can be implemented in `GATK indelRealign` task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Additional alignment issues\n",
    "\n",
    "We'd better separate chromosome BAM files in order to parallelly process them easier.\n",
    "\n",
    "Moreover, for unmapped reads, we'd better conduct a further realignment to avoid missing due to reference incompatibillity or incompleteness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BAM/SAM\n",
    "\n",
    "SAM (Sequence Alignment/Map format) is an unified format for storing NGS alignment against the reference genome.\n",
    "\n",
    "Here are the some features for SAM/BAM format:\n",
    "- SAM file stores the alignment information from most alignment tools;\n",
    "- SAM supports almost all the sequencing technologies;\n",
    "- SAM supports indexing for quick retrieval;\n",
    "- Reads in the SAM file can be classified into logical groups (e.g., lanes, libraries, or individuals)\n",
    "\n",
    "The BAM (binary alignment/map format) is the binary equivalence of SAM.\n",
    "\n",
    "### 5.1 SAM format\n",
    "\n",
    "A typical SAM file contains two sections - the HEADER section and the ALIGNMENT section.\n",
    "\n",
    "The ALIGNMENT section stores the alignment information for the reads with each one line. Each line contains 11 mandatory fields and several optional fields (formatted as TAG TYPE VALUE). Here is the table for the list of mandatory fields:\n",
    "\n",
    "| Col | Field | Type | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 |  QNAME | str | query name of the read or the read pair |\n",
    "| 2 | FLAG | int | bitwise flags (pairing, mapped, mate mapped, etc.) |\n",
    "| 3 | RNAME | str | reference sequence name |\n",
    "| 4 | POS | int | 1-based leftmost position of clipped alignment |\n",
    "| 5 | MAPQ | int | PHRED-scale mapping quality |\n",
    "| 6 | CIGAR | str | extended CIGAR string for alignment details |\n",
    "| 7 | RNEXT | str | mate reference name ('=' for the same) |\n",
    "| 8 | PNEXT | int | position of mate/next segment |\n",
    "| 9 | TLEN | int | observed template length |\n",
    "| 10 | SEQ | str | segment sequence |\n",
    "| 11 | QUAL | str | ASCII of PHRED-scale base quality | \n",
    "\n",
    "For further about SAM/BAM format, please refer to https://samtools.github.io/hts-specs/SAMv1.pdf. \n",
    "\n",
    "For FLAG information, you can refer to http://picard.sourceforge.net/explain-flags.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SAM/BAM file processing tools\n",
    "- **`samtools`**: C program and library\n",
    "    * http://samtools.sourceforge.net\n",
    "    * `samtools view`: SAM-BAM conversion\n",
    "    * `samtools sort`: sort the SAM records according to the positions\n",
    "    * `samtools index`:  create the index for SAM records\n",
    "    * `samtools merge`: merge  multiple BAM files\n",
    "    * `samtools flagstats`: summarizes the mapping flags\n",
    "    \n",
    "- **`Picard`**: Java program suite\n",
    "    * http://picard.sourceforge.net\n",
    "    * `MarkDuplicates`, `CollectAlignmentSummaryMetrics`, `CreateSequeceDictionary`, `SamToFastq`, `MeanQualityByCycle`\n",
    "\n",
    "- **`Pysam`**: Python interface to samtools\n",
    "    * http://code.google.com/p/pysam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variant Calling\n",
    "\n",
    "The vaiant calling step will generate a great deal of calling\n",
    "- Single nucleotide variants (SNVs) with genotype information (homozygous or heterozygous)\n",
    "- indels and\n",
    "- structural variants (SVs)\n",
    "\n",
    "This is a list of variant calling tools:\n",
    "- [samtools](http://samtools.sourceforge.net), [bcftools](http://bcftools.sourceforge.net)\n",
    "- [GATK](http://gatk.broadinstitute.org), [SOAPsnp](http://soap.genomics.org.cn/soapsnp.html), [Dindel](https://sites.google.com/site/keesalbers/soft/dindel)\n",
    "- [SVMerge](http://svmerge.sourceforge.net)\n",
    "\n",
    "These calling tools will generate a resulting file in [VCF](https://samtools.github.io/hts-specs/VCFv4.2.pdf) or [pileup](samtools.sourceforge.net/pileup.shtml).\n",
    "\n",
    "The calling and the filtering protocols will take advantages of the depth, quality, strand bias as well as the multiple sample information to accomplish the calling task.\n",
    "\n",
    "Compared to SNPs, the indels and the structural variants are much harder to call.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
